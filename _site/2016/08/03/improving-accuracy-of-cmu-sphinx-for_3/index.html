<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">

<!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Improving the Accuracy of CMU Sphinx for a Limited Vocabulary | nixing around</title>
<meta property="og:title" content="Improving the Accuracy of CMU Sphinx for a Limited Vocabulary" />
<meta name="author" content="umhau" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Update:&nbsp;I finished my tool for creating a customized voice model. &nbsp;It encapsulates the best of what I described below. &nbsp;See here:&nbsp;https://github.com/umhau/vmc.The idea with a limited vocabulary is that the processor can deal with far less information in order to detect the words needed. You don&#39;t have to train it on a complete set of words in the English language, and you don&#39;t need a supercomputer. &nbsp;All you have to do is teach it a few words, and how to spell them. &nbsp;The tutorial is here. &nbsp;I&#39;ve created a script to automate the voice recording here, and stashed the needed files there with it. PreparationAlright, down to business. &nbsp;You&#39;ll find it handy to keep a folder for these sorts of programs.mkdir ~/toolscd ~/toolsInstall git, if you don&#39;t have it already.sudo apt-get install gitDownload the script I made into your new tools folder.sudo git clone https://github.com/umhau/train-voice-data-pocketsphinx.gitInstall SphinxTrain. &nbsp;I included it among the files you just downloaded. &nbsp;Move it up to ~/tools, extract and install it. &nbsp;It&#39;s also here, if you don&#39;t want to use the one I provided.sudo mv ~/tools/train-voice-data-pocketsphinx/extra_files/sphinxtrain-5prealpha.tar.gz ~/toolssudo tar -xvzf ~/tools/sphinxtrain-5prealpha.tar.gz -C ~/toolscd sphinxtrain-5prealpha./configuremake -j 4make -j 4 installRecord Your VoiceEnter this directory, run the script. &nbsp;It&#39;ll have a basic walkthrough built-in. &nbsp;This will help you record the data you need. &nbsp;For experimental purposes, 20 recordings is enough for about 10% relative improvement in accuracy. &nbsp;Use the name neo-en for your training data, assuming you&#39;re working in English.cd ./train-voice-data-pocketsphinxpython train_voice_model.pyYou&#39;ll find your recordings in a subfolder with the same name as what you specified. &nbsp;Go there.cd ./neo-enBy the way, if you ever change your mind about what you want your model to be named, there&#39;s a fantastic program called pyrenamer that can make it easy to rename all the files you created. &nbsp;Install it with:sudo apt-get install pyrenamerProcess Your Voice RecordingsGreat! &nbsp;Done with that part. &nbsp;Now we&#39;re going to copy some other directories into the current working directory to &#39;work on them&#39;.cp -a /usr/local/share/pocketsphinx/model/en-us/en-us .cp -a /usr/local/share/pocketsphinx/model/en-us/cmudict-en-us.dict .cp -a /usr/local/share/pocketsphinx/model/en-us/en-us.lm.bin .Based on this source, it looks like we shouldn&#39;t be working with .dmp files. &nbsp; This is a point of deviation from the (outdated) CMU walkthrough. &nbsp;Copy the .bin file instead. &nbsp;Difference is explained below, sourced from the tutorial.Language model can be stored and loaded in three different format - text ARPA format, binary format BIN and binary DMP format. ARPA format takes more space but it is possible to edit it. ARPA files have .lm extension. Binary format takes significantly less space and faster to load. Binary files have .lm.bin extension. It is also possible to convert between formats. DMP format is obsolete and not recommended.Now, while still in this directory, generate some &#39;acoustic feature files&#39;.sphinx_fe -argfile en-us/feat.params -samprate 16000 -c neo-en.fileids -di . -do . -ei wav -eo mfc -mswav yesGet the Full-Sized Language ModelNice. &nbsp;You have a bunch more files with weird extensions on them. &nbsp;Now it&#39;s time to convert them. &nbsp;You need the full version of the language model, which was not shared with your original installation for size reasons. &nbsp;I included it in the github repository, or you can download it from here&nbsp;(you want the file named&nbsp;cmusphinx-en-us-ptm-5.2.tar.gz). &nbsp;Put the extracted files in your neo-en directory. Assuming you use the one from the github repo and you&#39;re still in the neo-en subdirectory,tar -xvzf ../extra_files/cmusphinx-en-us-ptm-5.2.tar.gz -C .There&#39;s an folder labeled en-us within the neo-en folder that was created when you made the acoustic feature files. &nbsp;Give it an extension and save it in case of horrible mistakes.mv ./en-us ./en-us-originalNow move the newly extracted directory to your neo-en folder, and rename it to en-us. mv ./cmusphinx-en-us-ptm-5.2 ./en-usThis converts the binary mdef file into a text file. pocketsphinx_mdef_convert -text ./en-us/mdef ./en-us/mdef.txtGrab Some ToolsNow you need some more tools to work with the data. &nbsp;These are from SphinxTrain, which you installed earlier. &nbsp;You should still be in your working directory, neo-en. &nbsp;Use ls to see what tools are available in the directory.ls /usr/local/libexec/sphinxtraincp /usr/local/libexec/sphinxtrain/bw .cp /usr/local/libexec/sphinxtrain/map_adapt .cp /usr/local/libexec/sphinxtrain/mk_s2sendump .cp /usr/local/libexec/sphinxtrain/mllr_solve .Run &#39;bw&#39; Command to Collect Statistics on Your VoiceNow you&#39;re going to run a very long command that is designed to collect statistics about your voice. &nbsp;Those backslashes -- the \ things -- tell bash to ignore the following character: in this case, newline characters. &nbsp;That&#39;s how this command is stretching over multiple lines. ./bw \ -hmmdir en-us \ -moddeffn en-us/mdef.txt \ -ts2cbfn .ptm. \ -feat 1s_c_d_dd \ -svspec 0-12/13-25/26-38 \ -cmn current \ -agc none \ -dictfn cmudict-en-us.dict \ -ctlfn neo-en.fileids \ -lsnfn neo-en.transcription \ -accumdir .Future note, for using the continuous model instead of the PTM model (from the tutorial):Make sure the arguments in bw command should match the parameters in feat.params file inside the acoustic model folder. Please note that not all the parameters from feat.param are supported by bw, only a few of them. bw for example doesn&#39;t suppport upperf or other feature extraction params. You only need to use parameters which are accepted, other parameters from feat.params should be skipped.&nbsp;For example, for continuous model you don&#39;t need to include the svspec option. Instead, you need to use just -ts2cbfn .cont. For semi-continuous models use -ts2cbfn .semi. If model has `feature_transform` file like en-us continuous model, you need to add -lda feature_transform argument to bw, otherwise it will not work properly.More CommandsNow it&#39;s time to adapt the model. &nbsp;Looks like continuous will be better to use in the long run, but first we&#39;re just going to get this working. &nbsp;The tutorial suggests that using MLLR and MAP adaptation methods together is best, but it looks like so far we&#39;re just using them sequentially. &nbsp;Here goes:./mllr_solve \ -meanfn en-us/means \ -varfn en-us/variances \ -outmllrfn mllr_matrix -accumdir .It appears this adapted model is now completed! &nbsp;Nice work. &nbsp;To use it, add&nbsp;-mllr mllr_matrix to your PocketSphinx command line. &nbsp;I&#39;ll put complete commands at the bottom of this note. &nbsp;Now we&#39;re going to do the MAP adaptation method, which is being used on top of the MLLR method. &nbsp;Back up the files you were just working on:cp -a en-us en-us-adaptTo run the MAP adaptation:./map_adapt \ -moddeffn en-us/mdef.txt \ -ts2cbfn .ptm. \ -meanfn en-us/means \ -varfn en-us/variances \ -mixwfn en-us/mixture_weights \ -tmatfn en-us/transition_matrices \ -accumdir . \ -mapmeanfn en-us-adapt/means \ -mapvarfn en-us-adapt/variances \ -mapmixwfn en-us-adapt/mixture_weights \ -maptmatfn en-us-adapt/transition_matrices[Optional; saves some space]...I think. &nbsp;Apparently it&#39;s now important to recreate a sendump file from a newly updated mixture_weights file../mk_s2sendump \ -pocketsphinx yes \ -moddeffn en-us-adapt/mdef.txt \ -mixwfn en-us-adapt/mixture_weights \ -sendumpfn en-us-adapt/sendumpTesting the ModelIt&#39;s also important to test the adaptation quality. &nbsp;This actually gives you a benchmark - a word error rate (WER). &nbsp;See here. Create Test DataUse another script I made to record test data. &nbsp;It&#39;s almost the same, but the fileids and transcription file formats are different. &nbsp;The folder with the test data should end up in the neo-en directory. Use the directory name I provide, test-data.python ../create_test_records.pyRun the decoder on the test files. &nbsp;Go back into the neo-en folder.pocketsphinx_batch \ -adcin yes \ -cepdir ./test-data \ -cepext .wav \ -ctl ./test-data/test-data.fileids \ -lm en-us.lm.bin \ -dict cmudict-en-us.dict \ -hmm en-us-adapt \ -hyp ./test-data/test-data.hypUse this tool to actually test the accuracy of the model. &nbsp;You&#39;ll need a working pocketsphinx installation, since it&#39;s just a wrapper with a word comparison engine over the transcription engine. &nbsp;Look at the end of the output; it&#39;ll give you some percentages indicating accuracy. ../../pocketsphinx-5prealpha/test/word_align.pl \ ./test-data/test-data.transcription \ ./test-data/test-data.hypLive TestingIf you just want to try out your new language model, record a file and try to transcribe it with these commands (assuming you&#39;re still in the neo-en working directory):python ../record_test_voice.pypocketsphinx_continuous -hmm ./en-us-adapt -infile ../test-file.wavOr, if you&#39;d rather use a microphone and record live, use this command:pocketsphinx_continuous -hmm ./en-us-adapt -inmic yesWith 110 voice records and using 20 records as testing, I achieved 60% accuracy. At 400 records and a marginal mic, I achieved 77% accuracy. &nbsp;There&#39;s about 1000 records available.Achieving Optimal AccuracyYou&#39;ll want to create your own language model if you&#39;re going to be using a specialized language. That&#39;s a pain, and you have to know what you&#39;re going to use it for ahead of time. &nbsp;If I do that, I&#39;ll collect the words from the tools where I specified them and automagically rebuild the language model. &nbsp;For now, I think I can get away with using the default lm.For actual use of the model, everything you need is in en-us-adapt. &nbsp;That&#39;s what you use when you need to refer in a command to your language-model. Use the following command to transcribe a file, if you&#39;ve created your own lm and language dictionary:pocketsphinx_continuous -hmm &lt;your_new_model_folder&gt; \ -lm &lt;your_lm&gt; \ -dict &lt;your_dict&gt; \ -infile test.wavUpon testing it appears that a less controlled environment might be useful, as the transcription was almost perfect when I was able to recreate the atmosphere of the original training records and pretty bad otherwise.ConclusionsI&#39;ve made a bunch of scripts in the github repo that automate some of this stuff, assuming standard installs. &nbsp;Look for check_accuracy.sh and create_model.sh. &nbsp;Everything should be run inside the neo-en folder, except the original train_voice_model.py script.TODO next -set up the more accurate continuous modelcreate a script that generates words in faux-sentences based on my use case scenario. &nbsp;find a phonetic dictionary that covers my needsfigure out what my use case actually is" />
<meta property="og:description" content="Update:&nbsp;I finished my tool for creating a customized voice model. &nbsp;It encapsulates the best of what I described below. &nbsp;See here:&nbsp;https://github.com/umhau/vmc.The idea with a limited vocabulary is that the processor can deal with far less information in order to detect the words needed. You don&#39;t have to train it on a complete set of words in the English language, and you don&#39;t need a supercomputer. &nbsp;All you have to do is teach it a few words, and how to spell them. &nbsp;The tutorial is here. &nbsp;I&#39;ve created a script to automate the voice recording here, and stashed the needed files there with it. PreparationAlright, down to business. &nbsp;You&#39;ll find it handy to keep a folder for these sorts of programs.mkdir ~/toolscd ~/toolsInstall git, if you don&#39;t have it already.sudo apt-get install gitDownload the script I made into your new tools folder.sudo git clone https://github.com/umhau/train-voice-data-pocketsphinx.gitInstall SphinxTrain. &nbsp;I included it among the files you just downloaded. &nbsp;Move it up to ~/tools, extract and install it. &nbsp;It&#39;s also here, if you don&#39;t want to use the one I provided.sudo mv ~/tools/train-voice-data-pocketsphinx/extra_files/sphinxtrain-5prealpha.tar.gz ~/toolssudo tar -xvzf ~/tools/sphinxtrain-5prealpha.tar.gz -C ~/toolscd sphinxtrain-5prealpha./configuremake -j 4make -j 4 installRecord Your VoiceEnter this directory, run the script. &nbsp;It&#39;ll have a basic walkthrough built-in. &nbsp;This will help you record the data you need. &nbsp;For experimental purposes, 20 recordings is enough for about 10% relative improvement in accuracy. &nbsp;Use the name neo-en for your training data, assuming you&#39;re working in English.cd ./train-voice-data-pocketsphinxpython train_voice_model.pyYou&#39;ll find your recordings in a subfolder with the same name as what you specified. &nbsp;Go there.cd ./neo-enBy the way, if you ever change your mind about what you want your model to be named, there&#39;s a fantastic program called pyrenamer that can make it easy to rename all the files you created. &nbsp;Install it with:sudo apt-get install pyrenamerProcess Your Voice RecordingsGreat! &nbsp;Done with that part. &nbsp;Now we&#39;re going to copy some other directories into the current working directory to &#39;work on them&#39;.cp -a /usr/local/share/pocketsphinx/model/en-us/en-us .cp -a /usr/local/share/pocketsphinx/model/en-us/cmudict-en-us.dict .cp -a /usr/local/share/pocketsphinx/model/en-us/en-us.lm.bin .Based on this source, it looks like we shouldn&#39;t be working with .dmp files. &nbsp; This is a point of deviation from the (outdated) CMU walkthrough. &nbsp;Copy the .bin file instead. &nbsp;Difference is explained below, sourced from the tutorial.Language model can be stored and loaded in three different format - text ARPA format, binary format BIN and binary DMP format. ARPA format takes more space but it is possible to edit it. ARPA files have .lm extension. Binary format takes significantly less space and faster to load. Binary files have .lm.bin extension. It is also possible to convert between formats. DMP format is obsolete and not recommended.Now, while still in this directory, generate some &#39;acoustic feature files&#39;.sphinx_fe -argfile en-us/feat.params -samprate 16000 -c neo-en.fileids -di . -do . -ei wav -eo mfc -mswav yesGet the Full-Sized Language ModelNice. &nbsp;You have a bunch more files with weird extensions on them. &nbsp;Now it&#39;s time to convert them. &nbsp;You need the full version of the language model, which was not shared with your original installation for size reasons. &nbsp;I included it in the github repository, or you can download it from here&nbsp;(you want the file named&nbsp;cmusphinx-en-us-ptm-5.2.tar.gz). &nbsp;Put the extracted files in your neo-en directory. Assuming you use the one from the github repo and you&#39;re still in the neo-en subdirectory,tar -xvzf ../extra_files/cmusphinx-en-us-ptm-5.2.tar.gz -C .There&#39;s an folder labeled en-us within the neo-en folder that was created when you made the acoustic feature files. &nbsp;Give it an extension and save it in case of horrible mistakes.mv ./en-us ./en-us-originalNow move the newly extracted directory to your neo-en folder, and rename it to en-us. mv ./cmusphinx-en-us-ptm-5.2 ./en-usThis converts the binary mdef file into a text file. pocketsphinx_mdef_convert -text ./en-us/mdef ./en-us/mdef.txtGrab Some ToolsNow you need some more tools to work with the data. &nbsp;These are from SphinxTrain, which you installed earlier. &nbsp;You should still be in your working directory, neo-en. &nbsp;Use ls to see what tools are available in the directory.ls /usr/local/libexec/sphinxtraincp /usr/local/libexec/sphinxtrain/bw .cp /usr/local/libexec/sphinxtrain/map_adapt .cp /usr/local/libexec/sphinxtrain/mk_s2sendump .cp /usr/local/libexec/sphinxtrain/mllr_solve .Run &#39;bw&#39; Command to Collect Statistics on Your VoiceNow you&#39;re going to run a very long command that is designed to collect statistics about your voice. &nbsp;Those backslashes -- the \ things -- tell bash to ignore the following character: in this case, newline characters. &nbsp;That&#39;s how this command is stretching over multiple lines. ./bw \ -hmmdir en-us \ -moddeffn en-us/mdef.txt \ -ts2cbfn .ptm. \ -feat 1s_c_d_dd \ -svspec 0-12/13-25/26-38 \ -cmn current \ -agc none \ -dictfn cmudict-en-us.dict \ -ctlfn neo-en.fileids \ -lsnfn neo-en.transcription \ -accumdir .Future note, for using the continuous model instead of the PTM model (from the tutorial):Make sure the arguments in bw command should match the parameters in feat.params file inside the acoustic model folder. Please note that not all the parameters from feat.param are supported by bw, only a few of them. bw for example doesn&#39;t suppport upperf or other feature extraction params. You only need to use parameters which are accepted, other parameters from feat.params should be skipped.&nbsp;For example, for continuous model you don&#39;t need to include the svspec option. Instead, you need to use just -ts2cbfn .cont. For semi-continuous models use -ts2cbfn .semi. If model has `feature_transform` file like en-us continuous model, you need to add -lda feature_transform argument to bw, otherwise it will not work properly.More CommandsNow it&#39;s time to adapt the model. &nbsp;Looks like continuous will be better to use in the long run, but first we&#39;re just going to get this working. &nbsp;The tutorial suggests that using MLLR and MAP adaptation methods together is best, but it looks like so far we&#39;re just using them sequentially. &nbsp;Here goes:./mllr_solve \ -meanfn en-us/means \ -varfn en-us/variances \ -outmllrfn mllr_matrix -accumdir .It appears this adapted model is now completed! &nbsp;Nice work. &nbsp;To use it, add&nbsp;-mllr mllr_matrix to your PocketSphinx command line. &nbsp;I&#39;ll put complete commands at the bottom of this note. &nbsp;Now we&#39;re going to do the MAP adaptation method, which is being used on top of the MLLR method. &nbsp;Back up the files you were just working on:cp -a en-us en-us-adaptTo run the MAP adaptation:./map_adapt \ -moddeffn en-us/mdef.txt \ -ts2cbfn .ptm. \ -meanfn en-us/means \ -varfn en-us/variances \ -mixwfn en-us/mixture_weights \ -tmatfn en-us/transition_matrices \ -accumdir . \ -mapmeanfn en-us-adapt/means \ -mapvarfn en-us-adapt/variances \ -mapmixwfn en-us-adapt/mixture_weights \ -maptmatfn en-us-adapt/transition_matrices[Optional; saves some space]...I think. &nbsp;Apparently it&#39;s now important to recreate a sendump file from a newly updated mixture_weights file../mk_s2sendump \ -pocketsphinx yes \ -moddeffn en-us-adapt/mdef.txt \ -mixwfn en-us-adapt/mixture_weights \ -sendumpfn en-us-adapt/sendumpTesting the ModelIt&#39;s also important to test the adaptation quality. &nbsp;This actually gives you a benchmark - a word error rate (WER). &nbsp;See here. Create Test DataUse another script I made to record test data. &nbsp;It&#39;s almost the same, but the fileids and transcription file formats are different. &nbsp;The folder with the test data should end up in the neo-en directory. Use the directory name I provide, test-data.python ../create_test_records.pyRun the decoder on the test files. &nbsp;Go back into the neo-en folder.pocketsphinx_batch \ -adcin yes \ -cepdir ./test-data \ -cepext .wav \ -ctl ./test-data/test-data.fileids \ -lm en-us.lm.bin \ -dict cmudict-en-us.dict \ -hmm en-us-adapt \ -hyp ./test-data/test-data.hypUse this tool to actually test the accuracy of the model. &nbsp;You&#39;ll need a working pocketsphinx installation, since it&#39;s just a wrapper with a word comparison engine over the transcription engine. &nbsp;Look at the end of the output; it&#39;ll give you some percentages indicating accuracy. ../../pocketsphinx-5prealpha/test/word_align.pl \ ./test-data/test-data.transcription \ ./test-data/test-data.hypLive TestingIf you just want to try out your new language model, record a file and try to transcribe it with these commands (assuming you&#39;re still in the neo-en working directory):python ../record_test_voice.pypocketsphinx_continuous -hmm ./en-us-adapt -infile ../test-file.wavOr, if you&#39;d rather use a microphone and record live, use this command:pocketsphinx_continuous -hmm ./en-us-adapt -inmic yesWith 110 voice records and using 20 records as testing, I achieved 60% accuracy. At 400 records and a marginal mic, I achieved 77% accuracy. &nbsp;There&#39;s about 1000 records available.Achieving Optimal AccuracyYou&#39;ll want to create your own language model if you&#39;re going to be using a specialized language. That&#39;s a pain, and you have to know what you&#39;re going to use it for ahead of time. &nbsp;If I do that, I&#39;ll collect the words from the tools where I specified them and automagically rebuild the language model. &nbsp;For now, I think I can get away with using the default lm.For actual use of the model, everything you need is in en-us-adapt. &nbsp;That&#39;s what you use when you need to refer in a command to your language-model. Use the following command to transcribe a file, if you&#39;ve created your own lm and language dictionary:pocketsphinx_continuous -hmm &lt;your_new_model_folder&gt; \ -lm &lt;your_lm&gt; \ -dict &lt;your_dict&gt; \ -infile test.wavUpon testing it appears that a less controlled environment might be useful, as the transcription was almost perfect when I was able to recreate the atmosphere of the original training records and pretty bad otherwise.ConclusionsI&#39;ve made a bunch of scripts in the github repo that automate some of this stuff, assuming standard installs. &nbsp;Look for check_accuracy.sh and create_model.sh. &nbsp;Everything should be run inside the neo-en folder, except the original train_voice_model.py script.TODO next -set up the more accurate continuous modelcreate a script that generates words in faux-sentences based on my use case scenario. &nbsp;find a phonetic dictionary that covers my needsfigure out what my use case actually is" />
<link rel="canonical" href="http://localhost:4000/2016/08/03/improving-accuracy-of-cmu-sphinx-for_3/" />
<meta property="og:url" content="http://localhost:4000/2016/08/03/improving-accuracy-of-cmu-sphinx-for_3/" />
<meta property="og:site_name" content="nixing around" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-08-03T17:36:00-04:00" />
<script type="application/ld+json">
{"name":null,"description":"Update:&nbsp;I finished my tool for creating a customized voice model. &nbsp;It encapsulates the best of what I described below. &nbsp;See here:&nbsp;https://github.com/umhau/vmc.The idea with a limited vocabulary is that the processor can deal with far less information in order to detect the words needed. You don&#39;t have to train it on a complete set of words in the English language, and you don&#39;t need a supercomputer. &nbsp;All you have to do is teach it a few words, and how to spell them. &nbsp;The tutorial is here. &nbsp;I&#39;ve created a script to automate the voice recording here, and stashed the needed files there with it. PreparationAlright, down to business. &nbsp;You&#39;ll find it handy to keep a folder for these sorts of programs.mkdir ~/toolscd ~/toolsInstall git, if you don&#39;t have it already.sudo apt-get install gitDownload the script I made into your new tools folder.sudo git clone https://github.com/umhau/train-voice-data-pocketsphinx.gitInstall SphinxTrain. &nbsp;I included it among the files you just downloaded. &nbsp;Move it up to ~/tools, extract and install it. &nbsp;It&#39;s also here, if you don&#39;t want to use the one I provided.sudo mv ~/tools/train-voice-data-pocketsphinx/extra_files/sphinxtrain-5prealpha.tar.gz ~/toolssudo tar -xvzf ~/tools/sphinxtrain-5prealpha.tar.gz -C ~/toolscd sphinxtrain-5prealpha./configuremake -j 4make -j 4 installRecord Your VoiceEnter this directory, run the script. &nbsp;It&#39;ll have a basic walkthrough built-in. &nbsp;This will help you record the data you need. &nbsp;For experimental purposes, 20 recordings is enough for about 10% relative improvement in accuracy. &nbsp;Use the name neo-en for your training data, assuming you&#39;re working in English.cd ./train-voice-data-pocketsphinxpython train_voice_model.pyYou&#39;ll find your recordings in a subfolder with the same name as what you specified. &nbsp;Go there.cd ./neo-enBy the way, if you ever change your mind about what you want your model to be named, there&#39;s a fantastic program called pyrenamer that can make it easy to rename all the files you created. &nbsp;Install it with:sudo apt-get install pyrenamerProcess Your Voice RecordingsGreat! &nbsp;Done with that part. &nbsp;Now we&#39;re going to copy some other directories into the current working directory to &#39;work on them&#39;.cp -a /usr/local/share/pocketsphinx/model/en-us/en-us .cp -a /usr/local/share/pocketsphinx/model/en-us/cmudict-en-us.dict .cp -a /usr/local/share/pocketsphinx/model/en-us/en-us.lm.bin .Based on this source, it looks like we shouldn&#39;t be working with .dmp files. &nbsp; This is a point of deviation from the (outdated) CMU walkthrough. &nbsp;Copy the .bin file instead. &nbsp;Difference is explained below, sourced from the tutorial.Language model can be stored and loaded in three different format - text ARPA format, binary format BIN and binary DMP format. ARPA format takes more space but it is possible to edit it. ARPA files have .lm extension. Binary format takes significantly less space and faster to load. Binary files have .lm.bin extension. It is also possible to convert between formats. DMP format is obsolete and not recommended.Now, while still in this directory, generate some &#39;acoustic feature files&#39;.sphinx_fe -argfile en-us/feat.params -samprate 16000 -c neo-en.fileids -di . -do . -ei wav -eo mfc -mswav yesGet the Full-Sized Language ModelNice. &nbsp;You have a bunch more files with weird extensions on them. &nbsp;Now it&#39;s time to convert them. &nbsp;You need the full version of the language model, which was not shared with your original installation for size reasons. &nbsp;I included it in the github repository, or you can download it from here&nbsp;(you want the file named&nbsp;cmusphinx-en-us-ptm-5.2.tar.gz). &nbsp;Put the extracted files in your neo-en directory. Assuming you use the one from the github repo and you&#39;re still in the neo-en subdirectory,tar -xvzf ../extra_files/cmusphinx-en-us-ptm-5.2.tar.gz -C .There&#39;s an folder labeled en-us within the neo-en folder that was created when you made the acoustic feature files. &nbsp;Give it an extension and save it in case of horrible mistakes.mv ./en-us ./en-us-originalNow move the newly extracted directory to your neo-en folder, and rename it to en-us. mv ./cmusphinx-en-us-ptm-5.2 ./en-usThis converts the binary mdef file into a text file. pocketsphinx_mdef_convert -text ./en-us/mdef ./en-us/mdef.txtGrab Some ToolsNow you need some more tools to work with the data. &nbsp;These are from SphinxTrain, which you installed earlier. &nbsp;You should still be in your working directory, neo-en. &nbsp;Use ls to see what tools are available in the directory.ls /usr/local/libexec/sphinxtraincp /usr/local/libexec/sphinxtrain/bw .cp /usr/local/libexec/sphinxtrain/map_adapt .cp /usr/local/libexec/sphinxtrain/mk_s2sendump .cp /usr/local/libexec/sphinxtrain/mllr_solve .Run &#39;bw&#39; Command to Collect Statistics on Your VoiceNow you&#39;re going to run a very long command that is designed to collect statistics about your voice. &nbsp;Those backslashes -- the \\ things -- tell bash to ignore the following character: in this case, newline characters. &nbsp;That&#39;s how this command is stretching over multiple lines. ./bw \\ -hmmdir en-us \\ -moddeffn en-us/mdef.txt \\ -ts2cbfn .ptm. \\ -feat 1s_c_d_dd \\ -svspec 0-12/13-25/26-38 \\ -cmn current \\ -agc none \\ -dictfn cmudict-en-us.dict \\ -ctlfn neo-en.fileids \\ -lsnfn neo-en.transcription \\ -accumdir .Future note, for using the continuous model instead of the PTM model (from the tutorial):Make sure the arguments in bw command should match the parameters in feat.params file inside the acoustic model folder. Please note that not all the parameters from feat.param are supported by bw, only a few of them. bw for example doesn&#39;t suppport upperf or other feature extraction params. You only need to use parameters which are accepted, other parameters from feat.params should be skipped.&nbsp;For example, for continuous model you don&#39;t need to include the svspec option. Instead, you need to use just -ts2cbfn .cont. For semi-continuous models use -ts2cbfn .semi. If model has `feature_transform` file like en-us continuous model, you need to add -lda feature_transform argument to bw, otherwise it will not work properly.More CommandsNow it&#39;s time to adapt the model. &nbsp;Looks like continuous will be better to use in the long run, but first we&#39;re just going to get this working. &nbsp;The tutorial suggests that using MLLR and MAP adaptation methods together is best, but it looks like so far we&#39;re just using them sequentially. &nbsp;Here goes:./mllr_solve \\ -meanfn en-us/means \\ -varfn en-us/variances \\ -outmllrfn mllr_matrix -accumdir .It appears this adapted model is now completed! &nbsp;Nice work. &nbsp;To use it, add&nbsp;-mllr mllr_matrix to your PocketSphinx command line. &nbsp;I&#39;ll put complete commands at the bottom of this note. &nbsp;Now we&#39;re going to do the MAP adaptation method, which is being used on top of the MLLR method. &nbsp;Back up the files you were just working on:cp -a en-us en-us-adaptTo run the MAP adaptation:./map_adapt \\ -moddeffn en-us/mdef.txt \\ -ts2cbfn .ptm. \\ -meanfn en-us/means \\ -varfn en-us/variances \\ -mixwfn en-us/mixture_weights \\ -tmatfn en-us/transition_matrices \\ -accumdir . \\ -mapmeanfn en-us-adapt/means \\ -mapvarfn en-us-adapt/variances \\ -mapmixwfn en-us-adapt/mixture_weights \\ -maptmatfn en-us-adapt/transition_matrices[Optional; saves some space]...I think. &nbsp;Apparently it&#39;s now important to recreate a sendump file from a newly updated mixture_weights file../mk_s2sendump \\ -pocketsphinx yes \\ -moddeffn en-us-adapt/mdef.txt \\ -mixwfn en-us-adapt/mixture_weights \\ -sendumpfn en-us-adapt/sendumpTesting the ModelIt&#39;s also important to test the adaptation quality. &nbsp;This actually gives you a benchmark - a word error rate (WER). &nbsp;See here. Create Test DataUse another script I made to record test data. &nbsp;It&#39;s almost the same, but the fileids and transcription file formats are different. &nbsp;The folder with the test data should end up in the neo-en directory. Use the directory name I provide, test-data.python ../create_test_records.pyRun the decoder on the test files. &nbsp;Go back into the neo-en folder.pocketsphinx_batch \\ -adcin yes \\ -cepdir ./test-data \\ -cepext .wav \\ -ctl ./test-data/test-data.fileids \\ -lm en-us.lm.bin \\ -dict cmudict-en-us.dict \\ -hmm en-us-adapt \\ -hyp ./test-data/test-data.hypUse this tool to actually test the accuracy of the model. &nbsp;You&#39;ll need a working pocketsphinx installation, since it&#39;s just a wrapper with a word comparison engine over the transcription engine. &nbsp;Look at the end of the output; it&#39;ll give you some percentages indicating accuracy. ../../pocketsphinx-5prealpha/test/word_align.pl \\ ./test-data/test-data.transcription \\ ./test-data/test-data.hypLive TestingIf you just want to try out your new language model, record a file and try to transcribe it with these commands (assuming you&#39;re still in the neo-en working directory):python ../record_test_voice.pypocketsphinx_continuous -hmm ./en-us-adapt -infile ../test-file.wavOr, if you&#39;d rather use a microphone and record live, use this command:pocketsphinx_continuous -hmm ./en-us-adapt -inmic yesWith 110 voice records and using 20 records as testing, I achieved 60% accuracy. At 400 records and a marginal mic, I achieved 77% accuracy. &nbsp;There&#39;s about 1000 records available.Achieving Optimal AccuracyYou&#39;ll want to create your own language model if you&#39;re going to be using a specialized language. That&#39;s a pain, and you have to know what you&#39;re going to use it for ahead of time. &nbsp;If I do that, I&#39;ll collect the words from the tools where I specified them and automagically rebuild the language model. &nbsp;For now, I think I can get away with using the default lm.For actual use of the model, everything you need is in en-us-adapt. &nbsp;That&#39;s what you use when you need to refer in a command to your language-model. Use the following command to transcribe a file, if you&#39;ve created your own lm and language dictionary:pocketsphinx_continuous -hmm &lt;your_new_model_folder&gt; \\ -lm &lt;your_lm&gt; \\ -dict &lt;your_dict&gt; \\ -infile test.wavUpon testing it appears that a less controlled environment might be useful, as the transcription was almost perfect when I was able to recreate the atmosphere of the original training records and pretty bad otherwise.ConclusionsI&#39;ve made a bunch of scripts in the github repo that automate some of this stuff, assuming standard installs. &nbsp;Look for check_accuracy.sh and create_model.sh. &nbsp;Everything should be run inside the neo-en folder, except the original train_voice_model.py script.TODO next -set up the more accurate continuous modelcreate a script that generates words in faux-sentences based on my use case scenario. &nbsp;find a phonetic dictionary that covers my needsfigure out what my use case actually is","author":{"@type":"Person","name":"umhau"},"@type":"BlogPosting","url":"http://localhost:4000/2016/08/03/improving-accuracy-of-cmu-sphinx-for_3/","publisher":null,"image":null,"headline":"Improving the Accuracy of CMU Sphinx for a Limited Vocabulary","dateModified":"2016-08-03T17:36:00-04:00","datePublished":"2016-08-03T17:36:00-04:00","sameAs":null,"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2016/08/03/improving-accuracy-of-cmu-sphinx-for_3/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" type="text/css" href="/assets/css/styles.css">
    <meta name="viewport" content="width=device-width">
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
    <script src="/js/infinite-jekyll.js"></script>

  </head>
  <body>
    <box class="wrapper">
      <header>
        <a href="http://localhost:4000"> <h1>*nixing around</h1> </a>
        <p>A programmer's journal of complicated projects.</p>
        <p class="view"><a href="">github</a> | <a href="http://localhost:4000/about">about</a> | <a href="http://localhost:4000/explore">explore</a></p>

        <p>testing</p>

      </header>
      <section>

      <div class="post">

    <header class="post-header">
        
    <h1 class="post-title">Improving the Accuracy of CMU Sphinx for a Limited Vocabulary </h1>
    <p 
        class="post-meta">Aug 3, 2016 • <a  href="/2016/08/03/improving-accuracy-of-cmu-sphinx-for_3/">permalink</a>
        <i>  • Linux • speech to text • success • Mint 17.3 • STT • audio transcription • CMU Sphinx • PocketSphinx 5prealpha</i> 
    </p>
    </header>

    <article class="post-content">
    <div dir="ltr" style="text-align: left;" trbidi="on"><br /><b>Update:</b>&nbsp;I finished my tool for creating a customized voice model. &nbsp;It encapsulates the best of what I described below. &nbsp;See here:&nbsp;<a href="https://github.com/umhau/vmc">https://github.com/umhau/vmc</a>.<br /><br />The idea with a limited vocabulary is that the processor can deal with far less information in order to detect the words needed. You don't have to train it on a complete set of words in the English language, and you don't need a supercomputer. &nbsp;All you have to do is teach it a few words, and how to spell them. &nbsp;The tutorial is <a href="http://cmusphinx.sourceforge.net/wiki/tutorialadapt" target="_blank">here</a>. &nbsp;I've created a script to automate the voice recording <a href="https://github.com/umhau/train-voice-data-pocketsphinx" target="_blank">here</a>, and stashed the needed files there with it. <br /><h3>Preparation</h3>Alright, down to business. &nbsp;You'll find it handy to keep a folder for these sorts of programs.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">mkdir ~/tools<br />cd ~/tools</pre>Install git, if you don't have it already.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">sudo apt-get install git</pre>Download the script I made into your new tools folder.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">sudo git clone https://github.com/umhau/train-voice-data-pocketsphinx.git</pre>Install SphinxTrain. &nbsp;I included it among the files you just downloaded. &nbsp;Move it up to ~/tools, extract and install it. &nbsp;It's also <a href="https://sourceforge.net/projects/cmusphinx/files/sphinxtrain/5prealpha/" target="_blank">here</a>, if you don't want to use the one I provided.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">sudo mv ~/tools/train-voice-data-pocketsphinx/extra_files/sphinxtrain-5prealpha.tar.gz ~/tools<br />sudo tar -xvzf ~/tools/sphinxtrain-5prealpha.tar.gz -C ~/tools<br />cd sphinxtrain-5prealpha<br />./configure<br />make -j 4<br />make -j 4 install</pre><h3>Record Your Voice</h3>Enter this directory, run the script. &nbsp;It'll have a basic walkthrough built-in. &nbsp;This will help you record the data you need. &nbsp;For experimental purposes, 20 recordings is enough for about 10% relative improvement in accuracy. &nbsp;Use the name neo-en for your training data, assuming you're working in English.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">cd ./train-voice-data-pocketsphinx<br />python train_voice_model.py</pre>You'll find your recordings in a subfolder with the same name as what you specified. &nbsp;Go there.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">cd ./neo-en</pre>By the way, if you ever change your mind about what you want your model to be named, there's a fantastic program called pyrenamer that can make it easy to rename all the files you created. &nbsp;Install it with:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">sudo apt-get install pyrenamer</pre><h3>Process Your Voice Recordings</h3>Great! &nbsp;Done with that part. &nbsp;Now we're going to copy some other directories into the current working directory to 'work on them'.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">cp -a /usr/local/share/pocketsphinx/model/en-us/en-us .<br />cp -a /usr/local/share/pocketsphinx/model/en-us/cmudict-en-us.dict .<br />cp -a /usr/local/share/pocketsphinx/model/en-us/en-us.lm.bin .</pre>Based on <a href="http://cmusphinx.sourceforge.net/wiki/tutoriallm" target="_blank">this source</a>, it looks like we shouldn't be working with .dmp files. &nbsp; This is a point of deviation from the (outdated) CMU walkthrough. &nbsp;Copy the .bin file instead. &nbsp;Difference is explained below, sourced from the <a href="http://cmusphinx.sourceforge.net/wiki/tutoriallm" target="_blank">tutorial</a>.<br /><blockquote class="tr_bq">Language model can be stored and loaded in three different format - text ARPA format, binary format BIN and binary DMP format. ARPA format takes more space but it is possible to edit it. ARPA files have .lm extension. Binary format takes significantly less space and faster to load. Binary files have .lm.bin extension. It is also possible to convert between formats. DMP format is obsolete and not recommended.</blockquote>Now, while still in this directory, generate some 'acoustic feature files'.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">sphinx_fe -argfile en-us/feat.params -samprate 16000 -c neo-en.fileids -di . -do . -ei wav -eo mfc -mswav yes</pre><h4>Get the Full-Sized Language Model</h4>Nice. &nbsp;You have a bunch more files with weird extensions on them. &nbsp;Now it's time to convert them. &nbsp;You need the full version of the language model, which was not shared with your original installation for size reasons. &nbsp;I included it in the github repository, or you can download it from <a href="https://sourceforge.net/projects/cmusphinx/files/Acoustic%20and%20Language%20Models/US%20English/" target="_blank">here</a>&nbsp;(you want the file named&nbsp;cmusphinx-en-us-ptm-5.2.tar.gz). &nbsp;Put the extracted files in your neo-en directory. <br /><br />Assuming you use the one from the github repo and you're still in the neo-en subdirectory,<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">tar -xvzf ../extra_files/cmusphinx-en-us-ptm-5.2.tar.gz -C .</pre>There's an folder labeled en-us within the neo-en folder that was created when you made the acoustic feature files. &nbsp;Give it an extension and save it in case of horrible mistakes.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">mv ./en-us ./en-us-original</pre>Now move the newly extracted directory to your neo-en folder, and rename it to en-us. <br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">mv ./cmusphinx-en-us-ptm-5.2 ./en-us</pre>This converts the binary mdef file into a text file. <br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">pocketsphinx_mdef_convert -text ./en-us/mdef ./en-us/mdef.txt</pre><h4>Grab Some Tools</h4>Now you need some more tools to work with the data. &nbsp;These are from SphinxTrain, which you installed earlier. &nbsp;You should still be in your working directory, neo-en. &nbsp;Use ls to see what tools are available in the directory.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">ls /usr/local/libexec/sphinxtrain<br />cp /usr/local/libexec/sphinxtrain/bw .<br />cp /usr/local/libexec/sphinxtrain/map_adapt .<br />cp /usr/local/libexec/sphinxtrain/mk_s2sendump .<br />cp /usr/local/libexec/sphinxtrain/mllr_solve .</pre><h4>Run 'bw' Command to Collect Statistics on Your Voice</h4>Now you're going to run a very long command that is designed to collect statistics about your voice. &nbsp;Those backslashes -- the \ things -- tell bash to ignore the following character: in this case, newline characters. &nbsp;That's how this command is stretching over multiple lines. <br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">./bw \<br /> -hmmdir en-us \<br /> -moddeffn en-us/mdef.txt \<br /> -ts2cbfn .ptm. \<br /> -feat 1s_c_d_dd \<br /> -svspec 0-12/13-25/26-38 \<br /> -cmn current \<br /> -agc none \<br /> -dictfn cmudict-en-us.dict \<br /> -ctlfn neo-en.fileids \<br /> -lsnfn neo-en.transcription \<br /> -accumdir .</pre>Future note, for using the <a href="http://cmusphinx.sourceforge.net/wiki/acousticmodeltypes" target="_blank">continuous model instead of the PTM model</a> (<a href="http://cmusphinx.sourceforge.net/wiki/tutorialadapt#accumulating_observation_counts" target="_blank">from the tutorial</a>):<br /><blockquote class="tr_bq">Make sure the arguments in bw command should match the parameters in feat.params file inside the acoustic model folder. Please note that not all the parameters from feat.param are supported by bw, only a few of them. bw for example doesn't suppport upperf or other feature extraction params. You only need to use parameters which are accepted, other parameters from feat.params should be skipped.&nbsp;</blockquote><blockquote class="tr_bq">For example, for continuous model you don't need to include the svspec option. Instead, you need to use just -ts2cbfn .cont. For semi-continuous models use -ts2cbfn .semi. If model has `feature_transform` file like en-us continuous model, you need to add -lda feature_transform argument to bw, otherwise it will not work properly.</blockquote><h4>More Commands</h4>Now it's time to adapt the model. &nbsp;Looks like continuous will be better to use in the long run, but first we're just going to get this working. &nbsp;The tutorial suggests that using MLLR and MAP adaptation methods together is best, but it looks like so far we're just using them sequentially. &nbsp;Here goes:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">./mllr_solve \<br /> -meanfn en-us/means \<br /> -varfn en-us/variances \<br /> -outmllrfn mllr_matrix -accumdir .<br /></pre><div>It appears this adapted model is now completed! &nbsp;Nice work. &nbsp;To use it, add&nbsp;-mllr mllr_matrix to your PocketSphinx command line. &nbsp;I'll put complete commands at the bottom of this note. &nbsp;</div><div><br /></div>Now we're going to do the MAP adaptation method, which is being used on top of the MLLR method. &nbsp;Back up the files you were just working on:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">cp -a en-us en-us-adapt</pre>To run the MAP adaptation:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">./map_adapt \<br /> -moddeffn en-us/mdef.txt \<br /> -ts2cbfn .ptm. \<br /> -meanfn en-us/means \<br /> -varfn en-us/variances \<br /> -mixwfn en-us/mixture_weights \<br /> -tmatfn en-us/transition_matrices \<br /> -accumdir . \<br /> -mapmeanfn en-us-adapt/means \<br /> -mapvarfn en-us-adapt/variances \<br /> -mapmixwfn en-us-adapt/mixture_weights \<br /> -maptmatfn en-us-adapt/transition_matrices</pre><h4>[Optional; saves some space]</h4>...I think. &nbsp;Apparently it's now important to recreate a sendump file from a newly updated mixture_weights file.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">./mk_s2sendump \<br /> -pocketsphinx yes \<br /> -moddeffn en-us-adapt/mdef.txt \<br /> -mixwfn en-us-adapt/mixture_weights \<br /> -sendumpfn en-us-adapt/sendump</pre><h3>Testing the Model</h3>It's also important to test the adaptation quality. &nbsp;This actually gives you a benchmark - a word error rate (WER). &nbsp;See <a href="http://cmusphinx.sourceforge.net/wiki/tutorialadapt#testing_the_adaptation" target="_blank">here</a>. <br /><h4>Create Test Data</h4>Use another script I made to record test data. &nbsp;It's almost the same, but the fileids and transcription file formats are different. &nbsp;The folder with the test data should end up in the neo-en directory. Use the directory name I provide, test-data.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">python ../create_test_records.py</pre>Run the decoder on the test files. &nbsp;Go back into the neo-en folder.<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">pocketsphinx_batch \<br /> -adcin yes \<br /> -cepdir ./test-data \<br /> -cepext .wav \<br /> -ctl ./test-data/test-data.fileids \<br /> -lm en-us.lm.bin \<br /> -dict cmudict-en-us.dict \<br /> -hmm en-us-adapt \<br /> -hyp ./test-data/test-data.hyp</pre>Use this tool to actually test the accuracy of the model. &nbsp;You'll need a working pocketsphinx installation, since it's just a wrapper with a word comparison engine over the transcription engine. &nbsp;Look at the end of the output; it'll give you some percentages indicating accuracy. <br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">../../pocketsphinx-5prealpha/test/word_align.pl \<br /> ./test-data/test-data.transcription \<br /> ./test-data/test-data.hyp</pre><h3>Live Testing</h3>If you just want to try out your new language model, record a file and try to transcribe it with these commands (assuming you're still in the neo-en working directory):<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="line-height: 19.6px;">python ../record_test_voice.py<br />pocketsphinx_continuous -hmm ./en-us-adapt -infile ../test-file.wav</span></span></pre>Or, if you'd rather use a microphone and record live, use this command:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;"><span style="color: #333333;"><span style="line-height: 19.6px;">pocketsphinx_continuous -hmm ./en-us-adapt</span></span><span style="color: #333333; line-height: 19.6px;"> </span><span style="background-color: transparent; line-height: 19.6px;"><span style="color: #333333;">-inmic yes</span></span></pre><span style="background-color: transparent; line-height: 19.6px;"><span style="color: #333333;"><span style="color: black; line-height: normal;">With 110 voice records and using 20 records as testing, I achieved 60% accuracy. At 400 records and a marginal mic, I achieved 77% accuracy. &nbsp;There's about 1000 records available.</span></span></span><br /><h3><span style="background-color: transparent; line-height: 19.6px;"><span style="color: #333333;">Achieving Optimal Accuracy</span></span></h3>You'll want to create your own language model if you're going to be using a specialized language. That's a pain, and you have to know what you're going to use it for ahead of time. &nbsp;If I do that, I'll collect the words from the tools where I specified them and automagically rebuild the language model. &nbsp;For now, I think I can get away with using the default lm.<br /><br />For actual use of the model, everything you need is in en-us-adapt. &nbsp;That's what you use when you need to refer in a command to your language-model. <br /><br />Use the following command to transcribe a file, if you've created your own lm and language dictionary:<br /><pre style="background-color: #eff0f1; border: 0px; margin-bottom: 1em; max-height: 600px; overflow: auto; padding: 5px; width: auto; word-wrap: normal;">pocketsphinx_continuous <br /> -hmm &lt;your_new_model_folder&gt; \<br /> -lm &lt;your_lm&gt; \<br /> -dict &lt;your_dict&gt; \<br /> -infile test.wav</pre>Upon testing it appears that a less controlled environment might be useful, as the transcription was almost perfect when I was able to recreate the atmosphere of the original training records and pretty bad otherwise.<br /><h3>Conclusions</h3>I've made a bunch of scripts in the github repo that automate some of this stuff, assuming standard installs. &nbsp;Look for check_accuracy.sh and create_model.sh. &nbsp;Everything should be run inside the neo-en folder, except the original train_voice_model.py script.<br /><br />TODO next -<br /><ol style="text-align: left;"><li>set up the more accurate continuous model</li><li>create a script that generates words in faux-sentences based on my use case scenario. &nbsp;</li><ol><li>find a phonetic dictionary that covers my needs</li><li>figure out what my use case actually is</li></ol></ol><br /></div>
    </article>

    <p class="extra-space"></p>
    <hr>
    <p class="extra-space"></p>

</div>





      </section>
      <footer>
        <p><small>Hosted on GitHub &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a> and <a href="https://github.com/umhau">umhau</a></small></p>
      </footer>
    </box>
    <script src="/assets/js/scale.fix.js"></script>


  



  </body>
</html>
